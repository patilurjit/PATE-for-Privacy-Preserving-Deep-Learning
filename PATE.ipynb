{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i_s3ks7F2KNp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd44_wEl2MVN",
        "outputId": "d14ca303-79cb-4c77-d169-0ec290ec5d14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_transformer = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_data = datasets.MNIST('/content/drive/MyDrive/ESL/data',\n",
        "                            train = True,\n",
        "                            transform = data_transformer,\n",
        "                            target_transform = None,\n",
        "                            download = True)\n",
        "\n",
        "test_data = datasets.MNIST('/content/drive/MyDrive/ESL/data',\n",
        "                            train = False,\n",
        "                            transform = data_transformer,\n",
        "                            target_transform = None,\n",
        "                            download = True)"
      ],
      "metadata": {
        "id": "BTjqPzTn2ODF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8qYDC0M2eT6",
        "outputId": "7c48ed05-ba5f-4093-a927-ae8c74e4f3bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: /content/drive/MyDrive/ESL/data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5,), std=(0.5,))\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl4hGgH92hjI",
        "outputId": "00177db1-e667-4d7c-8a29-3f6ae3e5d41e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: /content/drive/MyDrive/ESL/data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5,), std=(0.5,))\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_teachers = 200\n",
        "batch_size = 50"
      ],
      "metadata": {
        "id": "xdwIk-dV2U-U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_data_loaders(data, n_teach):\n",
        "    data_loaders = []\n",
        "    data_len = len(train_data) // n_teach\n",
        "\n",
        "    for i in range(data_len):\n",
        "        indices = list(range(i*data_len, (i+1)*data_len))\n",
        "        sub_data = Subset(train_data, indices)\n",
        "        loader = torch.utils.data.DataLoader(sub_data, batch_size = batch_size)\n",
        "        data_loaders.append(loader)\n",
        "\n",
        "    return data_loaders"
      ],
      "metadata": {
        "id": "Xmra_sSD2l0q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_loaders = make_data_loaders(train_data, n_teachers)"
      ],
      "metadata": {
        "id": "R3FweqF13cgt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_student = Subset(test_data, list(range(9000)))\n",
        "test_data_student = Subset(test_data, list(range(9000, 10000)))\n",
        "\n",
        "train_loader_student = torch.utils.data.DataLoader(train_data_student, batch_size=batch_size)\n",
        "test_loader_student = torch.utils.data.DataLoader(test_data_student, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "S9aaWzwj3lKv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
        "        self.c2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
        "        self.c2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.c1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.c2_drop(self.c2(x)), 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training = self.training)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "RD1EgOzl30r_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer(model, trainloader, criterion, optim, epochs = 10):\n",
        "    cum_loss = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        for data, labels in trainloader:\n",
        "            optim.zero_grad()\n",
        "            pred = model.forward(data)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            cum_loss += loss.item()"
      ],
      "metadata": {
        "id": "7lXRr6XD5FsG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, dataloader):\n",
        "    outputs = torch.zeros(0, dtype = torch.long)\n",
        "    model.eval()\n",
        "\n",
        "    for data, labels in dataloader:\n",
        "        pred = model.forward(data)\n",
        "        vote = torch.argmax(torch.exp(pred), dim = 1)\n",
        "        outputs = torch.cat((outputs, vote))\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "yy3fRCFO5yCf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_trainer(num_teachers):\n",
        "    model_list = []\n",
        "\n",
        "    for i in range(num_teachers):\n",
        "        model = MNISTClassifier()\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "        trainer(model, teacher_loaders[i], criterion, optimizer)\n",
        "        model_list.append(model)\n",
        "\n",
        "    return model_list"
      ],
      "metadata": {
        "id": "EqAuyCOl6RJ0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = model_trainer(n_teachers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs9mq8Ss66MA",
        "outputId": "d3cd0b13-a0d1-4b75-ff6e-5a1806d9589f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-e61a157b442f>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.2"
      ],
      "metadata": {
        "id": "RyDvDgGe69_t"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def noisy_aggregation(teacher_models, dataloader, gamma):\n",
        "\n",
        "    preds = torch.torch.zeros((len(teacher_models), 9000), dtype = torch.long)\n",
        "\n",
        "    for i, model in enumerate(teacher_models):\n",
        "        res = predict(model, dataloader)\n",
        "        preds[i] = res\n",
        "\n",
        "    labels = np.array([]).astype(int)\n",
        "\n",
        "    for j in np.transpose(preds):\n",
        "        n_labels = np.bincount(j, minlength = 10)\n",
        "\n",
        "        beta = 1 / gamma\n",
        "        for k in range(len(n_labels)):\n",
        "            n_labels[k] += np.random.laplace(0, beta, 1)\n",
        "\n",
        "        vote = np.argmax(n_labels)\n",
        "        labels = np.append(labels, vote)\n",
        "\n",
        "    return preds.numpy(), labels"
      ],
      "metadata": {
        "id": "2HcltM-M7d1q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_models = models\n",
        "preds, student_labels = noisy_aggregation(teacher_models, train_loader_student, gamma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FJf5lCo8yMa",
        "outputId": "edb75127-f69b-496a-baa7-ffd7b6a997e9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-e61a157b442f>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def student_data_generator(student_train_loader, labels):\n",
        "    for batch_index, (data, _) in enumerate(iter(student_train_loader)):\n",
        "        start_index = batch_index * len(data)\n",
        "        end_index = (batch_index + 1) * len(data)\n",
        "        yield data, torch.from_numpy(labels[start_index:end_index])"
      ],
      "metadata": {
        "id": "nmg-6vjc87UQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_student = MNISTClassifier()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model_student.parameters(), lr = 0.001)\n",
        "epochs = 10\n",
        "steps = 0\n",
        "cum_loss = 0"
      ],
      "metadata": {
        "id": "_tsoZnul-NkE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(epochs):\n",
        "    model_student.train()\n",
        "    train_loader = student_data_generator(train_loader_student, student_labels)\n",
        "    for input, labels in train_loader:\n",
        "        steps += 1\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model_student.forward(input)\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cum_loss += loss.item()\n",
        "\n",
        "        if steps % 50 == 0:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            model_student.eval()\n",
        "            with torch.no_grad():\n",
        "                for input, labels in test_loader_student:\n",
        "                    log_vote = model_student(input)\n",
        "                    test_loss += criterion(log_vote, labels).item()\n",
        "\n",
        "                    ps = torch.exp(log_vote)\n",
        "                    top_p, top_class = ps.topk(1, dim = 1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            model_student.train()\n",
        "\n",
        "            print(\"Epoch: {}/{} | \".format(e+1, epochs),\n",
        "                  \"Train Loss: {:.3f} | \".format(cum_loss / len(train_loader_student)),\n",
        "                  \"Test Loss: {:.3f} | \".format(test_loss / len(test_loader_student)),\n",
        "                  \"Accuracy: {:.3f}\".format(accuracy / len(test_loader_student)))\n",
        "\n",
        "            cum_loss = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSzQo1Gw-cw5",
        "outputId": "26f9d784-c90a-4206-e7a6-bf2bd141ccbc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-e61a157b442f>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10 |  Train Loss: 0.566 |  Test Loss: 1.349 |  Accuracy: 0.654\n",
            "Epoch: 1/10 |  Train Loss: 0.301 |  Test Loss: 0.661 |  Accuracy: 0.790\n",
            "Epoch: 1/10 |  Train Loss: 0.187 |  Test Loss: 0.449 |  Accuracy: 0.866\n",
            "Epoch: 2/10 |  Train Loss: 0.153 |  Test Loss: 0.353 |  Accuracy: 0.904\n",
            "Epoch: 2/10 |  Train Loss: 0.144 |  Test Loss: 0.374 |  Accuracy: 0.886\n",
            "Epoch: 2/10 |  Train Loss: 0.120 |  Test Loss: 0.355 |  Accuracy: 0.901\n",
            "Epoch: 2/10 |  Train Loss: 0.100 |  Test Loss: 0.338 |  Accuracy: 0.906\n",
            "Epoch: 3/10 |  Train Loss: 0.111 |  Test Loss: 0.302 |  Accuracy: 0.912\n",
            "Epoch: 3/10 |  Train Loss: 0.119 |  Test Loss: 0.319 |  Accuracy: 0.906\n",
            "Epoch: 3/10 |  Train Loss: 0.086 |  Test Loss: 0.312 |  Accuracy: 0.902\n",
            "Epoch: 4/10 |  Train Loss: 0.082 |  Test Loss: 0.288 |  Accuracy: 0.921\n",
            "Epoch: 4/10 |  Train Loss: 0.103 |  Test Loss: 0.300 |  Accuracy: 0.915\n",
            "Epoch: 4/10 |  Train Loss: 0.099 |  Test Loss: 0.322 |  Accuracy: 0.913\n",
            "Epoch: 4/10 |  Train Loss: 0.074 |  Test Loss: 0.294 |  Accuracy: 0.920\n",
            "Epoch: 5/10 |  Train Loss: 0.078 |  Test Loss: 0.277 |  Accuracy: 0.921\n",
            "Epoch: 5/10 |  Train Loss: 0.099 |  Test Loss: 0.292 |  Accuracy: 0.920\n",
            "Epoch: 5/10 |  Train Loss: 0.070 |  Test Loss: 0.302 |  Accuracy: 0.920\n",
            "Epoch: 5/10 |  Train Loss: 0.056 |  Test Loss: 0.292 |  Accuracy: 0.925\n",
            "Epoch: 6/10 |  Train Loss: 0.094 |  Test Loss: 0.294 |  Accuracy: 0.920\n",
            "Epoch: 6/10 |  Train Loss: 0.096 |  Test Loss: 0.284 |  Accuracy: 0.920\n",
            "Epoch: 6/10 |  Train Loss: 0.059 |  Test Loss: 0.293 |  Accuracy: 0.917\n",
            "Epoch: 7/10 |  Train Loss: 0.064 |  Test Loss: 0.278 |  Accuracy: 0.924\n",
            "Epoch: 7/10 |  Train Loss: 0.092 |  Test Loss: 0.310 |  Accuracy: 0.915\n",
            "Epoch: 7/10 |  Train Loss: 0.076 |  Test Loss: 0.321 |  Accuracy: 0.915\n",
            "Epoch: 7/10 |  Train Loss: 0.061 |  Test Loss: 0.316 |  Accuracy: 0.918\n",
            "Epoch: 8/10 |  Train Loss: 0.079 |  Test Loss: 0.292 |  Accuracy: 0.920\n",
            "Epoch: 8/10 |  Train Loss: 0.088 |  Test Loss: 0.286 |  Accuracy: 0.921\n",
            "Epoch: 8/10 |  Train Loss: 0.065 |  Test Loss: 0.290 |  Accuracy: 0.913\n",
            "Epoch: 9/10 |  Train Loss: 0.058 |  Test Loss: 0.284 |  Accuracy: 0.920\n",
            "Epoch: 9/10 |  Train Loss: 0.080 |  Test Loss: 0.262 |  Accuracy: 0.935\n",
            "Epoch: 9/10 |  Train Loss: 0.072 |  Test Loss: 0.285 |  Accuracy: 0.919\n",
            "Epoch: 9/10 |  Train Loss: 0.065 |  Test Loss: 0.275 |  Accuracy: 0.923\n",
            "Epoch: 10/10 |  Train Loss: 0.058 |  Test Loss: 0.297 |  Accuracy: 0.918\n",
            "Epoch: 10/10 |  Train Loss: 0.077 |  Test Loss: 0.302 |  Accuracy: 0.912\n",
            "Epoch: 10/10 |  Train Loss: 0.062 |  Test Loss: 0.311 |  Accuracy: 0.912\n",
            "Epoch: 10/10 |  Train Loss: 0.054 |  Test Loss: 0.303 |  Accuracy: 0.916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_data_dependent_epsilon(teacher_predictions, private_labels):\n",
        "  np_labels = []\n",
        "\n",
        "  for i in np.transpose(teacher_predictions):\n",
        "          lab_freq = np.bincount(i, minlength = 10)\n",
        "          vote = np.argmax(lab_freq)\n",
        "          np_labels.append(vote)\n",
        "\n",
        "  print(f'The data dependent epsilon for {n_teachers} teachers and laplace parameter {1/gamma} is {np.max(np.abs(np_labels - private_labels))}.')"
      ],
      "metadata": {
        "id": "SX8vXszYxe3Y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_data_dependent_epsilon(preds, student_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu7-tCDxySPf",
        "outputId": "3f30a18d-d4bd-4131-93a5-14bf771fe2cc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data dependent epsilon for 200 teachers and laplace parameter 5.0 is 7.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MTB7SabI632E"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}